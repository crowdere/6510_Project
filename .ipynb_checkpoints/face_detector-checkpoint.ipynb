{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cmake\n",
    "# !pip install face_recognition\n",
    "# !pip install numpy\n",
    "# !pip install dlib\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code forked and tweaked from:\n",
    "- https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py\n",
    "\n",
    "to extend, just add more people into the known_people folder\n",
    "\n",
    "Detectron2 detection:\n",
    "- https://gilberttanner.com/blog/detectron-2-object-detection-with-pytorch\n",
    "- https://github.com/facebookresearch/detectron2\n",
    "\n",
    "OpenCV human detection:\n",
    "- https://thedatafrog.com/en/articles/human-detection-video/\n",
    "\n",
    "Yolo object detection:\n",
    "- https://pjreddie.com/darknet/yolo/\n",
    "- https://medium.com/@luanaebio/detecting-people-with-yolo-and-opencv-5c1f9bc6a810\n",
    "\n",
    "OpenCV eye detection:\n",
    "- https://github.com/stepacool/Eye-Tracker/blob/No_GUI/track.py\n",
    "- https://medium.com/@stepanfilonov/tracking-your-eyes-with-python-3952e66194a6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectron Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detectron2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "## Unsafe workaround\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Detection with Detectron 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_detectron2():    \n",
    "    # Create config\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"./detectron2-master/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "    cfg.MODEL.DEVICE = 'cpu'\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl\"\n",
    "\n",
    "    # Create predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return predictor, cfg\n",
    "\n",
    "def make_detectron2_prediction(frame, predictor):\n",
    "    frame = cv2.resize(frame, (480, 640))\n",
    "    return predictor(frame)\n",
    "\n",
    "def draw_detectron2_result(prediction, frame, cfg):\n",
    "    person_instances = prediction['instances'][np.where(prediction['instances'].pred_classes == 0)]\n",
    "    v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    return v.draw_instance_predictions(person_instances.to(\"cpu\"))\n",
    "\n",
    "def detectron_unleashed(frame, detectron_predictor, detectron_cfg, itr):\n",
    "    if itr%2 == 0:\n",
    "        detectron_prediction = make_detectron2_prediction(frame, detectron_predictor)\n",
    "        detectron_viz = draw_detectron2_result(detectron_prediction, frame, detectron_cfg).get_image()[:, :, ::-1]\n",
    "    else:\n",
    "        detectron_viz = frame\n",
    "    display_results_detectron(face_locations, face_names, frame, detectron_viz)\n",
    "    itr += 1\n",
    "    return itr\n",
    "\n",
    "def display_results_detectron(face_locations, face_names, frame, detectron_viz):\n",
    "    \"\"\"\n",
    "    Displaying results\n",
    "    return: None\n",
    "    \"\"\"\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "    detectron_img = cv2.resize(detectron_viz, (frame.shape[1], frame.shape[0]))\n",
    "    frame_final = cv2.bitwise_or(detectron_img, frame)\n",
    "    cv2.imshow('Video', frame_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human detection with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_human_classifier():\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    return hog\n",
    "\n",
    "def make_human_prediction(frame, hog):\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(12,12), scale = 1.03)\n",
    "    return np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "def draw_detection_results(frame, boxes):\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB),\n",
    "                          (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Humaniod', (xA,yA), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Detection with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def get_yolo_classes():\n",
    "    classes = None\n",
    "    with open('coco.names', 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    return classes\n",
    "\n",
    "def init_yolo():\n",
    "    net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    return net\n",
    "\n",
    "\n",
    "def make_yolo_prediction(frame, net):\n",
    "    net.setInput(cv2.dnn.blobFromImage(frame, 0.00392, (416,416), (0,0,0), True, crop=False))\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    outs = net.forward(output_layers)\n",
    "    return outs\n",
    "\n",
    "\n",
    "def extract_box(detection, Width, Height):\n",
    "    center_x = int(detection[0] * Width)\n",
    "    center_y = int(detection[1] * Height)\n",
    "    w = int(detection[2] * Width)\n",
    "    h = int(detection[3] * Height)\n",
    "    x = center_x - w / 2\n",
    "    y = center_y - h / 2\n",
    "    return x, y, w, h\n",
    "\n",
    "\n",
    "def list_to_dict(lst):\n",
    "    res_dct = {str(i): lst[0][i] for i in range(0, len(lst[0]))}\n",
    "    return res_dct\n",
    "\n",
    "\n",
    "def get_bounding_box(classes, outs, frame):\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    Width = frame.shape[1]\n",
    "    Height = frame.shape[0]\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.1:\n",
    "                x, y, w, h = extract_box(detection, Width, Height)\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "    return boxes, confidences, class_ids, class_id\n",
    "\n",
    "\n",
    "def draw_yolo_result(frame, boxes, confidences, class_ids, class_id):\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.1, 0.1)\n",
    "    #check if is people detection\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        if class_ids[i]==0:\n",
    "            label = str(classes[class_id]) \n",
    "            cv2.rectangle(frame, (round(box[0]),round(box[1])), \n",
    "                          (round(box[0]+box[2]),round(box[1]+box[3])), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (round(box[0])-10,round(box[1])-10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "def yolo_main(frame, process_this_frame, yolo_net):\n",
    "    if not process_this_frame:\n",
    "        yolo_pred = make_yolo_prediction(frame, yolo_net)\n",
    "        boxes, confidences, class_ids, class_id = get_bounding_box(classes, yolo_pred, frame)\n",
    "        frame = draw_yolo_result(frame, boxes, confidences, class_ids, class_id)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaze Detection with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_eye_detector():\n",
    "    detector_params = cv2.SimpleBlobDetector_Params()\n",
    "    detector_params.filterByArea = True\n",
    "    detector_params.maxArea = 1500\n",
    "    detector = cv2.SimpleBlobDetector_create(detector_params)\n",
    "    return cv2.CascadeClassifier('haarcascade_eye.xml'), detector\n",
    "\n",
    "def crop_face(img, x, y, w, h):\n",
    "    return img[y:y + h, x:x + w]\n",
    "\n",
    "\n",
    "def detect_eyes(img, cascade):\n",
    "    gray_frame = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    eyes = cascade.detectMultiScale(gray_frame, 1.03, 5)  # detect eyes\n",
    "    width = np.size(img, 1)  # get face frame width\n",
    "    height = np.size(img, 0)  # get face frame height\n",
    "    left_eye = None\n",
    "    right_eye = None\n",
    "    for (x, y, w, h) in eyes:\n",
    "        if y > height / 2:\n",
    "            pass\n",
    "        eyecenter = x + w / 2  # get the eye center\n",
    "        if eyecenter < width * 0.5:\n",
    "            left_eye = img[y:y + h, x:x + w]\n",
    "        else:\n",
    "            right_eye = img[y:y + h, x:x + w]\n",
    "        \n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    \n",
    "    return left_eye, right_eye\n",
    "\n",
    "\n",
    "def cut_eyebrows(img):\n",
    "    height, width = img.shape[:2]\n",
    "    eyebrow_h = int(height / 4)\n",
    "    img = img[eyebrow_h:height, 0:width]  # cut eyebrows out (15 px)\n",
    "    return img\n",
    "\n",
    "\n",
    "def blob_process(img, threshold, detector):\n",
    "    gray_frame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, img = cv2.threshold(gray_frame, threshold, 255, cv2.THRESH_BINARY)\n",
    "    img = cv2.erode(img, None, iterations=2)\n",
    "    img = cv2.dilate(img, None, iterations=4)\n",
    "    img = cv2.medianBlur(img, 5)\n",
    "    keypoints = detector.detect(img)\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "def eye_detection_main(frame, eye_cascade, face_locations, process_this_frame, blob_detect):\n",
    "    if not process_this_frame:\n",
    "        for (y, w, h, x) in face_locations:\n",
    "            cropped_face = crop_face(frame, x*4, y*4, w*2, h) ## as done before in face recog display results\n",
    "            if cropped_face is not None:\n",
    "                eyes = detect_eyes(cropped_face, eye_cascade)\n",
    "                threshold = r = cv2.getTrackbarPos('threshold', 'video')\n",
    "                for eye in eyes:\n",
    "                    if eye is not None:\n",
    "                        eye = cut_eyebrows(eye)\n",
    "                        keypoints = blob_process(eye, threshold, blob_detect)\n",
    "                        eye = cv2.drawKeypoints(eye, keypoints, eye, (0, 0, 255),\n",
    "                                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "                        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    \"\"\"\n",
    "    Make an array of all the images\n",
    "    return: list, int, \n",
    "    \"\"\"\n",
    "    list_of_files = [f for f in glob(path+'*.jpg')]\n",
    "    number_files = len(list_of_files)\n",
    "    names = list_of_files.copy()\n",
    "    return list_of_files, number_files, names\n",
    "\n",
    "def get_known_faces(number_files, list_of_files, names):\n",
    "    \"\"\"\n",
    "    get known names and face encodings\n",
    "    return: two lists\n",
    "    \"\"\"\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    for i in range(number_files):\n",
    "        globals()['image_{}'.format(i)] = face_recognition.load_image_file(list_of_files[i])\n",
    "        globals()['image_encoding_{}'.format(i)] = face_recognition.face_encodings(globals()['image_{}'.format(i)])[0]\n",
    "        known_face_encodings.append(globals()['image_encoding_{}'.format(i)])\n",
    "        names[i] = names[i].replace(\"known_people/\", \"\")  \n",
    "        known_face_names.append(names[i])\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "def get_face_names(face_encodings, known_face_encodings, known_face_names):\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        # name = first_match(matches, known_face_names)\n",
    "        name = known_face_match(known_face_encodings, known_face_names, face_encoding, matches)\n",
    "        face_names.append(name)\n",
    "    return face_names\n",
    "\n",
    "def process_frame(process_this_frame, rgb_small_frame, face_names, face_locations):\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "        face_names = get_face_names(face_encodings, known_face_encodings, known_face_names)\n",
    "    process_this_frame = not process_this_frame\n",
    "    return face_names, face_locations, process_this_frame\n",
    "\n",
    "def display_results(face_locations, face_names, frame):\n",
    "    \"\"\"\n",
    "    Displaying results\n",
    "    return: None\n",
    "    \"\"\"\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "    return frame\n",
    "    \n",
    "def first_match(matches, known_face_names):\n",
    "    # If a match was found in known_face_encodings\n",
    "    if True in matches:\n",
    "        first_match_index = matches.index(True)\n",
    "        name = known_face_names[first_match_index]\n",
    "        return name\n",
    "    return 'Unknown'\n",
    "    \n",
    "def known_face_match(known_face_encodings, known_face_names, face_encoding, matches):\n",
    "    # use the known face with the smallest distance to the new face\n",
    "    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "    best_match_index = np.argmin(face_distances)\n",
    "    if matches[best_match_index]:\n",
    "        name = known_face_names[best_match_index]\n",
    "        return name\n",
    "    return 'Unknown'\n",
    "\n",
    "def notify(title, text):\n",
    "    os.system(\"\"\"\n",
    "              osascript -e 'display notification \"{}\" with title \"{}\"'\n",
    "              \"\"\".format(text, title))\n",
    "    \n",
    "def notify_reset_timer(notification_timer, name):\n",
    "    if(notification_timer <= 0 and name.lower() == \"unknown\"):\n",
    "        notification_timer = 60\n",
    "        notify(\"Shoulder Surfing Detected\", \"Quick behind you!\")\n",
    "    else:\n",
    "        notification_timer-=1\n",
    "    return notification_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# make array of sample pictures with encodings\n",
    "dirname = os.path.dirname(\"__file__\")\n",
    "path = os.path.join(dirname, 'known_people/')\n",
    "\n",
    "## Initializing OpenCV classifier\n",
    "# hog = initialize_human_classifier()\n",
    "\n",
    "# ## initializing detectron2\n",
    "# detectron_predictor, detectron_cfg = initialize_detectron2()\n",
    "\n",
    "## Initializing the yolo net\n",
    "yolo_net = init_yolo()\n",
    "eye_net, blob_detect = initialize_eye_detector()\n",
    "\n",
    "## Reading images\n",
    "list_of_files, number_files, names = read_images(path)\n",
    "\n",
    "known_face_encodings, known_face_names = get_known_faces(number_files,\n",
    "                                                         list_of_files,\n",
    "                                                         names)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "notification_timer = 0\n",
    "\n",
    "itr = 0\n",
    "\n",
    "classes = get_yolo_classes()\n",
    "\n",
    "cv2.namedWindow('video')\n",
    "cv2.createTrackbar('threshold', 'video', 0, 255, nothing)\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "    # Only process every other frame of video to save time\n",
    "    face_names, face_locations, process_this_frame = process_frame(process_this_frame, \n",
    "                                                                   rgb_small_frame, \n",
    "                                                                   face_names,\n",
    "                                                                   face_locations)\n",
    "    \n",
    "    frame = display_results(face_locations, face_names, frame)\n",
    "\n",
    "    frame = yolo_main(frame, process_this_frame, yolo_net)\n",
    "    \n",
    "    frame = eye_detection_main(frame, eye_net, face_locations, process_this_frame, blob_detect)\n",
    "    cv2.imshow('video', frame)\n",
    "    #check for unknown entiites and alert user every ~30 seconds ish\n",
    "    if face_names !=[]:\n",
    "        notification_timer = notify_reset_timer(notification_timer, face_names[-1])\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "1. Gaze Detection (pose detection)\n",
    "2. Consequence addition (notifications ignored)\n",
    "3. Deranking - lose access\n",
    "4. Threats - known attacks on opencv\n",
    "5. lock the screen if main user is not there\n",
    "6. Possible GUI\n",
    "7. Build admin backend to keep track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-404eb4fe7f38>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-404eb4fe7f38>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    video_capture = cv2.VideoCapture(0)a\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#code forked and tweaked from https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py\n",
    "#to extend, just add more people into the known_people folder\n",
    "\n",
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def notify(title, text):\n",
    "    os.system(\"\"\"\n",
    "              osascript -e 'display notification \"{}\" with title \"{}\"'\n",
    "              \"\"\".format(text, title))\n",
    "    \n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "#make array of sample pictures with encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "dirname = os.path.dirname(\"__file__\")\n",
    "path = os.path.join(dirname, 'known_people/')\n",
    "\n",
    "#make an array of all the saved jpg files' paths\n",
    "list_of_files = [f for f in glob.glob(path+'*.jpg')]\n",
    "#find number of known faces\n",
    "number_files = len(list_of_files)\n",
    "\n",
    "names = list_of_files.copy()\n",
    "\n",
    "for i in range(number_files):\n",
    "    globals()['image_{}'.format(i)] = face_recognition.load_image_file(list_of_files[i])\n",
    "    globals()['image_encoding_{}'.format(i)] = face_recognition.face_encodings(globals()['image_{}'.format(i)])[0]\n",
    "    known_face_encodings.append(globals()['image_encoding_{}'.format(i)])\n",
    "\n",
    "    # Create array of known names\n",
    "    names[i] = names[i].replace(\"known_people/\", \"\")  \n",
    "    known_face_names.append(names[i])\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "notification_timer = 0\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    #check for unknown entiites and alert user every ~30 seconds ish\n",
    "    if(notification_timer <= 0 and name == \"Unknown\"):\n",
    "        notification_timer = 60\n",
    "        notify(\"Shoulder Surfing Detected\", \"Quick behind you!\")\n",
    "    else:\n",
    "        notification_timer-=1\n",
    "        \n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "614.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
